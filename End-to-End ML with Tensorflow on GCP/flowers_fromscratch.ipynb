{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Flowers Image Classification with TensorFlow on Cloud ML Engine\n",
    "\n",
    "This notebook demonstrates how to do image classification from scratch on a flowers dataset using the Estimator API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"qwiklabs-gcp-02-80d4be27465a\" # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = \"qwiklabs-gcp-02-80d4be27465a\" # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = \"us-central1\" # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "MODEL_TYPE = \"cnn\"\n",
    "\n",
    "# do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"MODEL_TYPE\"] = MODEL_TYPE\n",
    "os.environ[\"TFVERSION\"] = \"1.13\"  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Input functions to read JPEG images\n",
    "\n",
    "The key difference between this notebook and [the MNIST one](./mnist_models.ipynb) is in the input function.\n",
    "In the input function here, we are doing the following:\n",
    "* Reading JPEG images, rather than 2D integer arrays.\n",
    "* Reading in batches of batch_size images rather than slicing our in-memory structure to be batch_size images.\n",
    "* Resizing the images to the expected HEIGHT, WIDTH. Because this is a real-world dataset, the images are of different sizes. We need to preprocess the data to, at the very least, resize them to constant size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Run as a Python module\n",
    "\n",
    "Since we want to run our code on Cloud ML Engine, we've packaged it as a python module.\n",
    "\n",
    "The `model.py` and `task.py` containing the model code is in <a href=\"flowersmodel\">flowersmodel</a>\n",
    "\n",
    "**Complete the TODOs in `model.py` before proceeding!**\n",
    "\n",
    "Once you've completed the TODOs, run it locally for a few steps to test the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From flowersmodel/model.py:23: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From flowersmodel/model.py:23: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From flowersmodel/model.py:206: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
      "\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {}, u'job': {u'args': [u'--output_dir=/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/08_image/labs/flowers_trained', u'--train_steps=5', u'--learning_rate=0.01', u'--batch_size=2', u'--model=cnn', u'--augment', u'--train_data_path=gs://cloud-ml-data/img/flower_photos/train_set.csv', u'--eval_data_path=gs://cloud-ml-data/img/flower_photos/eval_set.csv'], u'job_name': u'flowersmodel.task'}, u'task': {}}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 300, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff897f88cd0>, '_model_dir': '/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/08_image/labs/flowers_trained/', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.decode_csv is deprecated. Please use tf.io.decode_csv instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.read_file is deprecated. Please use tf.io.read_file instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_bilinear is deprecated. Please use tf.compat.v1.image.resize_bilinear instead.\n",
      "\n",
      "WARNING:tensorflow:From flowersmodel/model.py:150: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From flowersmodel/model.py:64: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/layers/convolutional.py:424: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From flowersmodel/model.py:66: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From flowersmodel/model.py:83: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From flowersmodel/model.py:86: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From flowersmodel/model.py:175: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From flowersmodel/model.py:175: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From flowersmodel/model.py:179: The name tf.train.get_global_step is deprecated. Please use tf.compat.v1.train.get_global_step instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2019-12-30 03:59:21.725910: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-12-30 03:59:21.726332: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555a96aa7e60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2019-12-30 03:59:21.726381: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2019-12-30 03:59:21.726541: I tensorflow/core/common_runtime/process_util.cc:136] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/08_image/labs/flowers_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.5429158, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 5 into /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/08_image/labs/flowers_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From flowersmodel/model.py:185: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-30T03:59:32Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/08_image/labs/flowers_trained/model.ckpt-5\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-30-04:00:40\n",
      "INFO:tensorflow:Saving dict for global step 5: accuracy = 0.17567568, global_step = 5, loss = 2.152491\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5: /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/08_image/labs/flowers_trained/model.ckpt-5\n",
      "WARNING:tensorflow:From flowersmodel/model.py:123: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'classes']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/08_image/labs/flowers_trained/model.ckpt-5\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/08_image/labs/flowers_trained/export/exporter/temp-1577678440/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 1.8233573.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf flowersmodel.tar.gz flowers_trained\n",
    "gcloud ai-platform local train \\\n",
    "    --module-name=flowersmodel.task \\\n",
    "    --package-path=${PWD}/flowersmodel \\\n",
    "    -- \\\n",
    "    --output_dir=${PWD}/flowers_trained \\\n",
    "    --train_steps=5 \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --batch_size=2 \\\n",
    "    --model=$MODEL_TYPE \\\n",
    "    --augment \\\n",
    "    --train_data_path=gs://cloud-ml-data/img/flower_photos/train_set.csv \\\n",
    "    --eval_data_path=gs://cloud-ml-data/img/flower_photos/eval_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, let's do it on ML Engine. Note the --model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://qwiklabs-gcp-02-80d4be27465a/flowers/trained_cnn us-central1 flowers_cnn_191230_040131\n",
      "jobId: flowers_cnn_191230_040131\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "Job [flowers_cnn_191230_040131] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe flowers_cnn_191230_040131\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs flowers_cnn_191230_040131\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/flowers/trained_${MODEL_TYPE}\n",
    "JOBNAME=flowers_${MODEL_TYPE}_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ai-platform jobs submit training $JOBNAME \\\n",
    "    --region=$REGION \\\n",
    "    --module-name=flowersmodel.task \\\n",
    "    --package-path=${PWD}/flowersmodel \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --staging-bucket=gs://$BUCKET \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    -- \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=1000 \\\n",
    "    --learning_rate=0.01 \\\n",
    "    --batch_size=40 \\\n",
    "    --model=$MODEL_TYPE \\\n",
    "    --augment \\\n",
    "    --batch_norm \\\n",
    "    --train_data_path=gs://cloud-ml-data/img/flower_photos/train_set.csv \\\n",
    "    --eval_data_path=gs://cloud-ml-data/img/flower_photos/eval_set.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor training with TensorBoard\n",
    "\n",
    "To activate TensorBoard within the JupyterLab UI navigate to \"<b>File</b>\" - \"<b>New Launcher</b>\".   Then double-click the 'Tensorboard' icon on the bottom row.\n",
    "\n",
    "TensorBoard 1 will appear in the new tab.  Navigate through the three tabs to see the active TensorBoard.   The 'Graphs' and 'Projector' tabs offer very interesting information including the ability to replay the tests.\n",
    "\n",
    "You may close the TensorBoard tab when you are finished exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deploying and predicting with model\n",
    "\n",
    "Deploy the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"flowers\"\n",
    "MODEL_VERSION=${MODEL_TYPE}\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/flowers/trained_${MODEL_TYPE}/export/exporter | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "#gcloud ai-platform versions delete --quiet ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ai-platform models delete ${MODEL_NAME}\n",
    "gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To predict with the model, let's take one of the example images that is available on Google Cloud Storage <img src=\"http://storage.googleapis.com/cloud-ml-data/img/flower_photos/sunflowers/1022552002_2b93faf9e7_n.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The online prediction service expects images to be base64 encoded as described [here](https://cloud.google.com/ml-engine/docs/tensorflow/online-predict#binary_data_in_prediction_input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "IMAGE_URL=gs://cloud-ml-data/img/flower_photos/sunflowers/1022552002_2b93faf9e7_n.jpg\n",
    "\n",
    "# Copy the image to local disk.\n",
    "gsutil cp $IMAGE_URL flower.jpg\n",
    "\n",
    "# Base64 encode and create request message in json format.\n",
    "python -c 'import base64, sys, json; img = base64.b64encode(open(\"flower.jpg\", \"rb\").read()).decode(); print(json.dumps({\"image_bytes\":{\"b64\": img}}))' &> request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Send it to the prediction service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ai-platform predict \\\n",
    "    --model=flowers \\\n",
    "    --version=${MODEL_TYPE} \\\n",
    "    --json-instances=./request.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<pre>\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.py\n",
    "\n",
    "#!/usr/bin/env python\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(v = tf.logging.INFO)\n",
    "\n",
    "LIST_OF_LABELS = \"daisy,dandelion,roses,sunflowers,tulips\".split(',')\n",
    "HEIGHT = 299\n",
    "WIDTH = 299\n",
    "NUM_CHANNELS = 3\n",
    "NCLASSES = 5\n",
    "\n",
    "def linear_model(img, mode, hparams):\n",
    "    X = tf.reshape(tensor = img, shape = [-1,HEIGHT * WIDTH]) #flatten\n",
    "    ylogits = tf.layers.dense(input = X, units = NCLASSES, activation = None)\n",
    "    return ylogits, NCLASSES\n",
    "\n",
    "def dnn_model(img, mode, hparams):\n",
    "    X = tf.reshape(tensor = img, shape = [-1, HEIGHT * WIDTH]) #flatten\n",
    "    h1 = tf.layers.dense(input = X, units = 300, activation = tf.nn.relu)\n",
    "    h2 = tf.layers.dense(input = h1, units = 100, activation = tf.nn.relu)\n",
    "    h3 = tf.layers.dense(input = h2, units = 30, activation = tf.nn.relu)\n",
    "    ylogits = tf.layers.dense(input = h3, units = NCLASSES, activation = None)\n",
    "    return ylogits, NCLASSES\n",
    "\n",
    "def dnn_dropout_model(img, mode, hparams):\n",
    "    dprob = hparams.get(\"dprob\", 0.1)\n",
    "\n",
    "    X = tf.reshape(tensor = img, shape = [-1, HEIGHT * WIDTH]) #flatten\n",
    "    h1 = tf.layers.dense(input = X, units = 300, activation = tf.nn.relu)\n",
    "    h2 = tf.layers.dense(input = h1, units = 100, activation = tf.nn.relu)\n",
    "    h3 = tf.layers.dense(input = h2, units = 30, activation = tf.nn.relu)\n",
    "    h3d = tf.layers.dropout(inputs = h3, rate = dprob, training = (mode == tf.estimator.ModeKeys.TRAIN)) #only dropout when training\n",
    "    ylogits = tf.layers.dense(input = h3d, units = NCLASSES, activation = None)\n",
    "    return ylogits, NCLASSES\n",
    "\n",
    "def cnn_model(img, mode, hparams):\n",
    "    ksize1 = hparams.get(\"ksize1\", 5)\n",
    "    ksize2 = hparams.get(\"ksize2\", 5)\n",
    "    nfil1 = hparams.get(\"nfil1\", 10)\n",
    "    nfil2 = hparams.get(\"nfil2\", 20)\n",
    "    dprob = hparams.get(\"dprob\", 0.25)\n",
    "\n",
    "    c1 = tf.layers.conv2d(inputs = img, filters = nfil1,\n",
    "                          kernel_size = ksize1, strides = 1,\n",
    "                          padding = \"same\", activation = tf.nn.relu) # shape = (batch_size, HEIGHT, WIDTH, nfil1)\n",
    "    \n",
    "    p1 = tf.layers.max_pooling2d(inputs = c1, pool_size = 2, strides = 2) # shape = (batch_size, HEIGHT // 2, WIDTH // 2, nfil1)\n",
    "    \n",
    "    c2 = tf.layers.conv2d(inputs = p1, filters = nfil2,\n",
    "                          kernel_size = ksize2, strides = 1, \n",
    "                          padding = \"same\", activation = tf.nn.relu) # shape = (batch_size, HEIGHT // 2, WIDTH // 2, nfil2)\n",
    "    \n",
    "    p2 = tf.layers.max_pooling2d(inputs = c2, pool_size = 2, strides = 2) # shape = (batch_size, HEIGHT // 4, WIDTH // 4, nfil2)\n",
    "\n",
    "    outlen = p2.shape[1] * p2.shape[2] * p2.shape[3] # HEIGHT // 4 * WIDTH // 4 * nfil2\n",
    "    p2flat = tf.reshape(tensor = p2, shape = [-1, outlen]) # shape = (batch_size, HEIGHT // 4 * WIDTH // 4 * nfil2)\n",
    "\n",
    "    # Apply batch normalization\n",
    "    if hparams[\"batch_norm\"]:\n",
    "        h3 = tf.layers.dense(inputs = p2flat, units = 300, activation = None)\n",
    "        h3 = tf.layers.batch_normalization(inputs = h3, training = (mode == tf.estimator.ModeKeys.TRAIN)) # only batchnorm when training\n",
    "        h3 = tf.nn.relu(features = h3)\n",
    "    else:  \n",
    "        h3 = tf.layers.dense(inputs = p2flat, units = 300, activation = tf.nn.relu)\n",
    "  \n",
    "    # Apply dropout\n",
    "    h3d = tf.layers.dropout(inputs = h3, rate = dprob, training = (mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    ylogits = tf.layers.dense(inputs = h3d, units = NCLASSES, activation = None)\n",
    "  \n",
    "    # Apply batch normalization once more\n",
    "    if hparams[\"batch_norm\"]:\n",
    "        ylogits = tf.layers.batch_normalization(inputs = ylogits, training = (mode == tf.estimator.ModeKeys.TRAIN))\n",
    "\n",
    "    return ylogits, NCLASSES\n",
    "\n",
    "def read_and_preprocess_with_augment(image_bytes, label = None):\n",
    "    return read_and_preprocess(image_bytes, label, augment = True)\n",
    "    \n",
    "def read_and_preprocess(image_bytes, label = None, augment = False):\n",
    "    # Decode the image, end up with pixel values that are in the -1, 1 range    \n",
    "    image = tf.image.decode_jpeg(image_bytes, channels=NUM_CHANNELS) #TODO: decode contents into JPEG\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)    #TODO: convert JPEG tensor to floats between 0 and 1\n",
    "    image = tf.expand_dims(input = image, axis = 0) # resize_bilinear needs batches\n",
    "    \n",
    "    if augment:\n",
    "        #TODO: Add image augmentation functions\n",
    "        image=tf.image.resize_bilinear(image, [HEIGHT+10, WIDTH+10], align_corners=False)\n",
    "        image=tf.squeeze(image)\n",
    "        image=tf.image.random_crop(image, [HEIGHT,WIDTH,NUM_CHANNELS])\n",
    "        image=tf.image.random_brightness(image,max_delta=63.0/255)\n",
    "        image=tf.image.random_contrast(image,lower=0.2,upper=1.8)\n",
    "    else:\n",
    "        image = tf.image.resize_bilinear(images = image, size = [HEIGHT, WIDTH], align_corners = False)\n",
    "        image = tf.squeeze(input = image, axis = 0) # remove batch dimension\n",
    "        \n",
    "    # Pixel values are in range [0,1], convert to [-1,1]\n",
    "    image = tf.subtract(x = image, y = 0.5)\n",
    "    image = tf.multiply(x = image, y = 2.0)\n",
    "    return {\"image\": image}, label\n",
    "\n",
    "def serving_input_fn():\n",
    "    # Note: only handles one image at a time \n",
    "    feature_placeholders = {\"image_bytes\": tf.placeholder(dtype = tf.string, shape = [])}\n",
    "    image, _ = read_and_preprocess(tf.squeeze(input = feature_placeholders[\"image_bytes\"]))\n",
    "    image[\"image\"] = tf.expand_dims(image[\"image\"], axis = 0)\n",
    "    return tf.estimator.export.ServingInputReceiver(features = image, receiver_tensors = feature_placeholders)\n",
    "\n",
    "def make_input_fn(csv_of_filenames, batch_size, mode, augment=False):\n",
    "    def _input_fn(): \n",
    "        def decode_csv(csv_row):\n",
    "            filename, label = tf.decode_csv(records = csv_row, record_defaults = [[\"\"],[\"\"]])\n",
    "            image_bytes = tf.read_file(filename = filename)\n",
    "            return image_bytes, label\n",
    "        \n",
    "        # Create tf.data.dataset from filename\n",
    "        dataset = tf.data.TextLineDataset(filenames = csv_of_filenames).map(map_func = decode_csv)\n",
    "        \n",
    "        if augment:\n",
    "            dataset = dataset.map(read_and_preprocess_with_augment)    #TODO: map read_and_preprocess_with_augment\n",
    "        else:\n",
    "            dataset = dataset.map(read_and_preprocess)                  #TODO: map read_and_preprocess\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    " \n",
    "        dataset = dataset.repeat(count = num_epochs).batch(batch_size = batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return _input_fn\n",
    "    \n",
    "def image_classifier(features, labels, mode, params):\n",
    "    model_functions = {\n",
    "        \"linear\": linear_model,\n",
    "        \"dnn\": dnn_model,\n",
    "        \"dnn_dropout\": dnn_dropout_model,\n",
    "        \"cnn\": cnn_model}\n",
    "    model_function = model_functions[params[\"model\"]] \n",
    "    ylogits, nclasses = model_function(features[\"image\"], mode, params)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits = ylogits)\n",
    "    class_int = tf.cast(x = tf.argmax(input = ylogits, axis = 1), dtype = tf.uint8)\n",
    "    class_str = tf.gather(params = LIST_OF_LABELS, indices = tf.cast(x = class_int, dtype = tf.int32))\n",
    "  \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # Convert string label to int\n",
    "        labels_table = tf.contrib.lookup.index_table_from_tensor(mapping = tf.constant(value = LIST_OF_LABELS, dtype = tf.string))\n",
    "        labels = labels_table.lookup(keys = labels)\n",
    "\n",
    "        loss = tf.reduce_mean(input_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(logits = ylogits, labels = tf.one_hot(indices = labels, depth = NCLASSES)))\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # This is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(key = tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(control_inputs = update_ops):\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss = loss, \n",
    "                    global_step = tf.train.get_global_step(),\n",
    "                    learning_rate = params[\"learning_rate\"],\n",
    "                    optimizer = \"Adam\")\n",
    "            eval_metric_ops = None\n",
    "        else:\n",
    "            train_op = None\n",
    "            eval_metric_ops =  {\"accuracy\": tf.metrics.accuracy(labels = labels, predictions = class_int)}\n",
    "    else:\n",
    "        loss = None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    " \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = {\"probabilities\": probabilities, \n",
    "                       \"classid\": class_int, \n",
    "                       \"class\": class_str},\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = {\"classes\": tf.estimator.export.PredictOutput(\n",
    "            {\"probabilities\": probabilities, \n",
    "             \"classid\": class_int, \n",
    "             \"class\": class_str})}\n",
    "    )\n",
    "\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    \n",
    "    EVAL_INTERVAL = 300 # every 5 minutes\n",
    "    \n",
    "    # Instantiate base estimator class for custom model function\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn = image_classifier,\n",
    "        params = hparams,\n",
    "        config = tf.estimator.RunConfig(\n",
    "            save_checkpoints_secs = EVAL_INTERVAL),\n",
    "            model_dir = output_dir)\n",
    "    \n",
    "    # Set estimator\"s train_spec to use train_input_fn and train for so many steps\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = make_input_fn(\n",
    "            hparams[\"train_data_path\"],\n",
    "            hparams[\"batch_size\"],\n",
    "            mode = tf.estimator.ModeKeys.TRAIN,\n",
    "            augment = hparams[\"augment\"]),\n",
    "        max_steps = hparams[\"train_steps\"])\n",
    "\n",
    "    # Create exporter that uses serving_input_fn to create saved_model for serving\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name = \"exporter\", \n",
    "        serving_input_receiver_fn = serving_input_fn)\n",
    "\n",
    "    # Set estimator\"s eval_spec to use eval_input_fn and export saved_model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = make_input_fn(\n",
    "            hparams[\"eval_data_path\"],\n",
    "            hparams[\"batch_size\"],\n",
    "            mode = tf.estimator.ModeKeys.EVAL),\n",
    "        steps = None,\n",
    "        exporters = exporter,\n",
    "        start_delay_secs = EVAL_INTERVAL,\n",
    "        throttle_secs = EVAL_INTERVAL)\n",
    "\n",
    "    # Run train_and_evaluate loop\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator = estimator, \n",
    "        train_spec = train_spec, \n",
    "        eval_spec = eval_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
