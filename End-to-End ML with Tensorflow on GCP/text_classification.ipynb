{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Classification using TensorFlow/Keras on Cloud ML Engine </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using BigQuery\n",
    "<li> Creating a text classification model using the Estimator API with a Keras model\n",
    "<li> Training on Cloud ML Engine\n",
    "<li> Deploying the model\n",
    "<li> Predicting with model\n",
    "<li> Rerun with pre-trained embedding\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'qwiklabs-gcp-01-61c2854f79bf'\n",
    "PROJECT = 'qwiklabs-gcp-01-61c2854f79bf'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub. \n",
    "\n",
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various  sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataset from BigQuery \n",
    "\n",
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015. \n",
    "\n",
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Ask HN: What's your speciality, and what's you...</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Ask HN: Books with a high signal to noise ratio?</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td>Ask HN: Why can't I make as much as I make?</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://www.kickstarter.com/projects/carlosxcl...</td>\n",
       "      <td>Show HN: Code Cards, Like Texas hold 'em for p...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>Ask HN: Is it OK to submit job postings on HN?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>http://vancouver.en.craigslist.ca/van/roo/2035...</td>\n",
       "      <td>Best Roommate Ad Ever</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>https://github.com/Groundworkstech/Submicron</td>\n",
       "      <td>Deep-Submicron Backdoors</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>http://empowerunited.com/</td>\n",
       "      <td>Could this be the solution for the 99%?</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>Ask HN: Can I help you be more awesome today? ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td></td>\n",
       "      <td>Show HN: Free and Open Source book to teach Fi...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                                                      \n",
       "3  https://www.kickstarter.com/projects/carlosxcl...   \n",
       "4                                                      \n",
       "5  http://vancouver.en.craigslist.ca/van/roo/2035...   \n",
       "6       https://github.com/Groundworkstech/Submicron   \n",
       "7                          http://empowerunited.com/   \n",
       "8                                                      \n",
       "9                                                      \n",
       "\n",
       "                                               title  score  \n",
       "0  Ask HN: What's your speciality, and what's you...    260  \n",
       "1   Ask HN: Books with a high signal to noise ratio?    260  \n",
       "2        Ask HN: Why can't I make as much as I make?    262  \n",
       "3  Show HN: Code Cards, Like Texas hold 'em for p...     11  \n",
       "4     Ask HN: Is it OK to submit job postings on HN?     11  \n",
       "5                              Best Roommate Ad Ever     11  \n",
       "6                           Deep-Submicron Backdoors     11  \n",
       "7            Could this be the solution for the 99%?     11  \n",
       "8  Ask HN: Can I help you be more awesome today? ...     11  \n",
       "9  Show HN: Free and Open Source book to teach Fi...     11  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "query=\"\"\"\n",
    "SELECT\n",
    "  url, title, score\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  LENGTH(title) > 10\n",
    "  AND score > 10\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = bq.Query(query).execute().result().to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the url is http://mobile.nytimes.com/...., I want to be left with <i>nytimes</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>num_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>blogspot</td>\n",
       "      <td>41386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>github</td>\n",
       "      <td>36525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>techcrunch</td>\n",
       "      <td>30891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>youtube</td>\n",
       "      <td>30848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>28787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>medium</td>\n",
       "      <td>18422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>google</td>\n",
       "      <td>18235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>wordpress</td>\n",
       "      <td>17667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>arstechnica</td>\n",
       "      <td>13749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>wired</td>\n",
       "      <td>12841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source  num_articles\n",
       "0     blogspot         41386\n",
       "1       github         36525\n",
       "2   techcrunch         30891\n",
       "3      youtube         30848\n",
       "4      nytimes         28787\n",
       "5       medium         18422\n",
       "6       google         18235\n",
       "7    wordpress         17667\n",
       "8  arstechnica         13749\n",
       "9        wired         12841"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "  ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "  COUNT(title) AS num_articles\n",
    "FROM\n",
    "  `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "  REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "  AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "  source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df = bq.Query(query).execute().result().to_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>github</td>\n",
       "      <td>php bdd is now  nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>github</td>\n",
       "      <td>mpv video player 0.2 release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>github</td>\n",
       "      <td>show hn  re-thinking the business card with dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>github</td>\n",
       "      <td>update css js from chrome developer tool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>github</td>\n",
       "      <td>simple way to start development with flask usi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source                                              title\n",
       "0  github                              php bdd is now  nice \n",
       "1  github                       mpv video player 0.2 release\n",
       "2  github  show hn  re-thinking the business card with dr...\n",
       "3  github           update css js from chrome developer tool\n",
       "4  github  simple way to start development with flask usi..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "SELECT source, LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title FROM\n",
    "  (SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.'))[OFFSET(1)] AS source,\n",
    "    title\n",
    "  FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "  WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '.*://(.[^/]+)/'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "  )\n",
    "WHERE (source = 'github' OR source = 'nytimes' OR source = 'techcrunch')\n",
    "\"\"\"\n",
    "\n",
    "df = bq.Query(query + \" LIMIT 10\").execute().result().to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset).  \n",
    "\n",
    "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (See https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = bq.Query(query + \" AND ABS(MOD(FARM_FINGERPRINT(title), 4)) > 0\").execute().result().to_dataframe()\n",
    "evaldf  = bq.Query(query + \" AND ABS(MOD(FARM_FINGERPRINT(title), 4)) = 0\").execute().result().to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see that roughly 75% of the data is used for training, and 25% for evaluation. \n",
    "\n",
    "We can also see that within each dataset, the classes are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        27445\n",
       "techcrunch    23131\n",
       "nytimes       21586\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "github        9080\n",
       "techcrunch    7760\n",
       "nytimes       7201\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaldf['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "DATADIR='data/txtcls'\n",
    "shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "os.makedirs(DATADIR)\n",
    "traindf.to_csv( os.path.join(DATADIR,'train.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')\n",
    "evaldf.to_csv( os.path.join(DATADIR,'eval.tsv'), header=False, index=False, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "github\tthis guy just found out how to bypass adblocker\n",
      "github\tshow hn  dodo   command line task management for developers\n",
      "github\tshow hn  webservicemock   mock out external calls for local development\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/txtcls/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  24041 data/txtcls/eval.tsv\n",
      "  72162 data/txtcls/train.tsv\n",
      "  96203 total\n"
     ]
    }
   ],
   "source": [
    "!wc -l data/txtcls/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras Code\n",
    "\n",
    "Please explore the code in this <a href=\"txtclsmodel/trainer\">directory</a>: `model.py` contains the TensorFlow model and `task.py` parses command line arguments and launches off the training job.\n",
    "\n",
    "There are some TODOs in the `model.py`, **make sure to complete the TODOs before proceeding!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Locally\n",
    "Let's make sure the code compiles by running locally for a fraction of an epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage\n",
      "  Using cached https://files.pythonhosted.org/packages/d8/13/70354d66a87c92797a4c233c00615d69986e7d7ecfbe6c00c7c237c9d465/google_cloud_storage-1.24.1-py2.py3-none-any.whl\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/9e/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098/google_resumable_media-0.5.0-py2.py3-none-any.whl\n",
      "Collecting google-cloud-core<2.0dev,>=1.1.0 (from google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/93/54/8c5542c0df4c8a92c0b8736ee67abc4daa9ff3734a0119310c125673ac79/google_cloud_core-1.1.0-py2.py3-none-any.whl\n",
      "Collecting google-auth<2.0dev,>=1.9.0 (from google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl\n",
      "Collecting six (from google-resumable-media<0.6dev,>=0.5.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/65/26/32b8464df2a97e6dd1b656ed26b2c194606c16fe163c695a992b36c11cdf/six-1.13.0-py2.py3-none-any.whl\n",
      "Collecting google-api-core<2.0.0dev,>=1.14.0 (from google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/70/bd363339fc88a1c434655ba03171d46aa2e679e1452cc95a4a8679e76a7a/google_api_core-1.15.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<2.0dev,>=1.9.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2.0dev,>=1.9.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/2f/a6/30b0a0bef12283e83e58c1d6e7b5aabc7acfc4110df81a4471655d33e704/cachetools-3.1.1-py2.py3-none-any.whl\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2.0dev,>=1.9.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Collecting setuptools>=40.3.0 (from google-auth<2.0dev,>=1.9.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/f9/d3/955738b20d3832dfa3cd3d9b07e29a8162edb480bf988332f5e6e48ca444/setuptools-44.0.0-py2.py3-none-any.whl\n",
      "Collecting requests<3.0.0dev,>=2.18.0 (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl\n",
      "Collecting pytz (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/e7/f9/f0b53f88060247251bf481fa6ea62cd0d25bf1b11a87888e53ce5b7c8ad2/pytz-2019.3-py2.py3-none-any.whl\n",
      "Collecting futures>=3.2.0; python_version < \"3.2\" (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/d8/a6/f46ae3f1da0cd4361c344888f59ec2f5785e69c872e175a748ef6071cdb5/futures-3.3.0-py2-none-any.whl\n",
      "Collecting protobuf>=3.4.0 (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/13/5c/ba4572a4d952b8db68c4534168a6d2a946b354de5e2b779efb44d4d0b72c/protobuf-3.11.2-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.9.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl\n",
      "Collecting idna<2.9,>=2.5 (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/63/df50cac98ea0d5b006c55a399c3bf1db9da7b5a24de7890bc9cfd5dd9e99/certifi-2019.11.28-py2.py3-none-any.whl\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.1.0->google-cloud-storage)\n",
      "  Using cached https://files.pythonhosted.org/packages/b4/40/a9837291310ee1ccc242ceb6ebfd9eb21539649f193a7c8c86ba15b98539/urllib3-1.25.7-py2.py3-none-any.whl\n",
      "Installing collected packages: six, google-resumable-media, idna, chardet, certifi, urllib3, requests, pytz, futures, setuptools, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, protobuf, googleapis-common-protos, google-api-core, google-cloud-core, google-cloud-storage\n",
      "Successfully installed cachetools-3.1.1 certifi-2019.11.28 chardet-3.0.4 futures-3.3.0 google-api-core-1.15.0 google-auth-1.10.0 google-cloud-core-1.1.0 google-cloud-storage-1.24.1 google-resumable-media-0.5.0 googleapis-common-protos-1.6.0 idna-2.8 protobuf-3.11.2 pyasn1-0.4.8 pyasn1-modules-0.2.7 pytz-2019.3 requests-2.22.0 rsa-4.0 setuptools-44.0.0 six-1.13.0 urllib3-1.25.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "WARNING:tensorflow:From trainer/model.py:23: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From trainer/model.py:23: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From trainer/model.py:251: The name tf.summary.FileWriterCache is deprecated. Please use tf.compat.v1.summary.FileWriterCache instead.\n",
      "\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {}, u'job': {u'args': [u'--output_dir=/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained', u'--train_data_path=/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/data/txtcls/train.tsv', u'--eval_data_path=/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/data/txtcls/eval.tsv', u'--num_epochs=0.1'], u'job_name': u'trainer.task'}, u'task': {}}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/initializers.py:119: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Using the Keras model provided.\n",
      "2020-01-05 00:19:34.186633: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "User settings:\n",
      "\n",
      "   KMP_AFFINITY=granularity=fine,verbose,compact,1,0\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_SETTINGS=1\n",
      "   OMP_NUM_THREADS=4\n",
      "\n",
      "Effective settings:\n",
      "\n",
      "   KMP_ABORT_DELAY=0\n",
      "   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n",
      "   KMP_ALIGN_ALLOC=64\n",
      "   KMP_ALL_THREADPRIVATE=128\n",
      "   KMP_ATOMIC_MODE=2\n",
      "   KMP_BLOCKTIME=0\n",
      "   KMP_CPUINFO_FILE: value is not defined\n",
      "   KMP_DETERMINISTIC_REDUCTION=false\n",
      "   KMP_DEVICE_THREAD_LIMIT=2147483647\n",
      "   KMP_DISP_HAND_THREAD=false\n",
      "   KMP_DISP_NUM_BUFFERS=7\n",
      "   KMP_DUPLICATE_LIB_OK=false\n",
      "   KMP_FORCE_REDUCTION: value is not defined\n",
      "   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n",
      "   KMP_FORKJOIN_BARRIER='2,2'\n",
      "   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_FORKJOIN_FRAMES=true\n",
      "   KMP_FORKJOIN_FRAMES_MODE=3\n",
      "   KMP_GTID_MODE=3\n",
      "   KMP_HANDLE_SIGNALS=false\n",
      "   KMP_HOT_TEAMS_MAX_LEVEL=1\n",
      "   KMP_HOT_TEAMS_MODE=0\n",
      "   KMP_INIT_AT_FORK=true\n",
      "   KMP_INIT_WAIT=2048\n",
      "   KMP_ITT_PREPARE_DELAY=0\n",
      "   KMP_LIBRARY=throughput\n",
      "   KMP_LOCK_KIND=queuing\n",
      "   KMP_MALLOC_POOL_INCR=1M\n",
      "   KMP_NEXT_WAIT=1024\n",
      "   KMP_NUM_LOCKS_IN_BLOCK=1\n",
      "   KMP_PLAIN_BARRIER='2,2'\n",
      "   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_REDUCTION_BARRIER='1,1'\n",
      "   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n",
      "   KMP_SCHEDULE='static,balanced;guided,iterative'\n",
      "   KMP_SETTINGS=true\n",
      "   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n",
      "   KMP_STACKOFFSET=64\n",
      "   KMP_STACKPAD=0\n",
      "   KMP_STACKSIZE=4M\n",
      "   KMP_STORAGE_MAP=false\n",
      "   KMP_TASKING=2\n",
      "   KMP_TASKLOOP_MIN_TASKS=0\n",
      "   KMP_TASK_STEALING_CONSTRAINT=1\n",
      "   KMP_TEAMS_THREAD_LIMIT=4\n",
      "   KMP_TOPOLOGY_METHOD=all\n",
      "   KMP_USER_LEVEL_MWAIT=false\n",
      "   KMP_VERSION=false\n",
      "   KMP_WARNINGS=true\n",
      "   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n",
      "   OMP_ALLOCATOR=omp_default_mem_alloc\n",
      "   OMP_CANCELLATION=false\n",
      "   OMP_DEFAULT_DEVICE=0\n",
      "   OMP_DISPLAY_AFFINITY=false\n",
      "   OMP_DISPLAY_ENV=false\n",
      "   OMP_DYNAMIC=false\n",
      "   OMP_MAX_ACTIVE_LEVELS=2147483647\n",
      "   OMP_MAX_TASK_PRIORITY=0\n",
      "   OMP_NESTED=false\n",
      "   OMP_NUM_THREADS='4'\n",
      "   OMP_PLACES: value is not defined\n",
      "   OMP_PROC_BIND='intel'\n",
      "   OMP_SCHEDULE='static'\n",
      "   OMP_STACKSIZE=4M\n",
      "   OMP_TARGET_OFFLOAD=DEFAULT\n",
      "   OMP_THREAD_LIMIT=2147483647\n",
      "   OMP_TOOL=enabled\n",
      "   OMP_TOOL_LIBRARIES: value is not defined\n",
      "   OMP_WAIT_POLICY=PASSIVE\n",
      "   KMP_AFFINITY='verbose,warnings,respect,granularity=fine,compact,1,0'\n",
      "\n",
      "2020-01-05 00:19:34.234130: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2020-01-05 00:19:34.234677: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e7171099f0 executing computations on platform Host. Devices:\n",
      "2020-01-05 00:19:34.234719: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "OMP: Info #212: KMP_AFFINITY: decoding x2APIC ids.\n",
      "OMP: Info #210: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\n",
      "OMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\n",
      "OMP: Info #156: KMP_AFFINITY: 4 available OS procs\n",
      "OMP: Info #157: KMP_AFFINITY: Uniform topology\n",
      "OMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores/pkg x 2 threads/core (2 total cores)\n",
      "OMP: Info #214: KMP_AFFINITY: OS proc to physical thread map:\n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \n",
      "OMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2801 thread 0 bound to OS proc set 0\n",
      "2020-01-05 00:19:34.236855: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:97: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:97: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "2020-01-05 00:19:35.005589: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe86959fe50>, '_model_dir': '/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained', '_protocol': None, '_save_checkpoints_steps': 500, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_train_distribute': None, '_master': ''}\n",
      "WARNING:tensorflow:From trainer/model.py:105: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From trainer/model.py:105: The name tf.estimator.inputs.numpy_input_fn is deprecated. Please use tf.compat.v1.estimator.inputs.numpy_input_fn instead.\n",
      "\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 500 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Warm-starting with WarmStartSettings: WarmStartSettings(ckpt_to_initialize_from='/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/keras/keras_model.ckpt', vars_to_warm_start='.*', var_name_to_vocab_info={}, var_name_to_prev_var_name={})\n",
      "INFO:tensorflow:Warm-starting from: ('/home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/keras/keras_model.ckpt',)\n",
      "INFO:tensorflow:Warm-starting variables only in TRAINABLE_VARIABLES.\n",
      "INFO:tensorflow:Warm-started 7 variables.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py:1354: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py:875: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2860 thread 1 bound to OS proc set 1\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt.\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2822 thread 2 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2863 thread 4 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2864 thread 5 bound to OS proc set 1\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2862 thread 3 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2823 thread 6 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2865 thread 7 bound to OS proc set 3\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2866 thread 8 bound to OS proc set 0\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2867 thread 9 bound to OS proc set 1\n",
      "INFO:tensorflow:loss = 1.0840178, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 57 into /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-01-05T00:19:44Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt-57\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2894 thread 10 bound to OS proc set 2\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2895 thread 10 bound to OS proc set 2\n",
      "INFO:tensorflow:Finished evaluation at 2020-01-05-00:19:47\n",
      "INFO:tensorflow:Saving dict for global step 57: acc = 0.37843683, global_step = 57, loss = 1.1385156\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 57: /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt-57\n",
      "WARNING:tensorflow:From trainer/model.py:198: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/model.ckpt-57\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/training-data-analyst/courses/machine_learning/deepdive/09_sequence/labs/txtcls_trained/export/exporter/temp-1578183587/saved_model.pb\n",
      "OMP: Info #250: KMP_AFFINITY: pid 2801 tid 2861 thread 10 bound to OS proc set 2\n",
      "INFO:tensorflow:Loss for final step: 1.0022836.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## Make sure we have the latest version of Google Cloud Storage package\n",
    "pip install --upgrade google-cloud-storage\n",
    "rm -rf txtcls_trained\n",
    "gcloud ml-engine local train \\\n",
    "   --module-name=trainer.task \\\n",
    "   --package-path=${PWD}/txtclsmodel/trainer \\\n",
    "   -- \\\n",
    "   --output_dir=${PWD}/txtcls_trained \\\n",
    "   --train_data_path=${PWD}/data/txtcls/train.tsv \\\n",
    "   --eval_data_path=${PWD}/data/txtcls/eval.tsv \\\n",
    "   --num_epochs=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on the Cloud\n",
    "\n",
    "Let's first copy our training data to the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://data/txtcls/eval.tsv [Content-Type=text/tab-separated-values]...\n",
      "Copying file://data/txtcls/train.tsv [Content-Type=text/tab-separated-values]...\n",
      "\\ [2 files][  5.4 MiB/  5.4 MiB]                                                \n",
      "Operation completed over 2 objects/5.4 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp data/txtcls/*.tsv gs://${BUCKET}/txtcls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: txtcls_200105_002115\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CommandException: 1 files/objects could not be removed.\n",
      "WARNING: The `gcloud ml-engine` commands have been renamed and will soon be removed. Please use `gcloud ai-platform` instead.\n",
      "Job [txtcls_200105_002115] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe txtcls_200105_002115\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs txtcls_200105_002115\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/txtcls/trained_fromscratch\n",
    "JOBNAME=txtcls_$(date -u +%y%m%d_%H%M%S)\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    " --region=$REGION \\\n",
    " --module-name=trainer.task \\\n",
    " --package-path=${PWD}/txtclsmodel/trainer \\\n",
    " --job-dir=$OUTDIR \\\n",
    " --scale-tier=BASIC_GPU \\\n",
    " --runtime-version=$TFVERSION \\\n",
    " -- \\\n",
    " --output_dir=$OUTDIR \\\n",
    " --train_data_path=gs://${BUCKET}/txtcls/train.tsv \\\n",
    " --eval_data_path=gs://${BUCKET}/txtcls/eval.tsv \\\n",
    " --num_epochs=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor training with TensorBoard\n",
    "\n",
    "To activate TensorBoard within the JupyterLab UI navigate to \"<b>File</b>\" - \"<b>New Launcher</b>\".   Then double-click the 'Tensorboard' icon on the bottom row.\n",
    "\n",
    "TensorBoard 1 will appear in the new tab.  Navigate through the three tabs to see the active TensorBoard.   The 'Graphs' and 'Projector' tabs offer very interesting information including the ability to replay the tests.\n",
    "\n",
    "You may close the TensorBoard tab when you are finished exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "What accuracy did you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy trained model \n",
    "\n",
    "Once your training completes you will see your exported models in the output directory specified in Google Cloud Storage. \n",
    "\n",
    "You should see one model for each training checkpoint (default is every 1000 steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/txtcls/trained_fromscratch/export/exporter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take the last export and deploy it as a REST API using Google Cloud Machine Learning Engine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"txtcls\"\n",
    "MODEL_VERSION=\"v1_fromscratch\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/txtcls/trained_fromscratch/export/exporter/ | tail -1)\n",
    "#gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME} --quiet\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=$TFVERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Predictions\n",
    "\n",
    "Here are some actual hacker news headlines gathered from July 2018. These titles were not part of the training or evaluation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "techcrunch=[\n",
    "  'Uber shuts down self-driving trucks unit',\n",
    "  'Grover raises €37M Series A to offer latest tech products as a subscription',\n",
    "  'Tech companies can now bid on the Pentagon’s $10B cloud contract'\n",
    "]\n",
    "nytimes=[\n",
    "  '‘Lopping,’ ‘Tips’ and the ‘Z-List’: Bias Lawsuit Explores Harvard’s Admissions',\n",
    "  'A $3B Plan to Turn Hoover Dam into a Giant Battery',\n",
    "  'A MeToo Reckoning in China’s Workplace Amid Wave of Accusations'\n",
    "]\n",
    "github=[\n",
    "  'Show HN: Moon – 3kb JavaScript UI compiler',\n",
    "  'Show HN: Hello, a CLI tool for managing social media',\n",
    "  'Firefox Nightly added support for time-travel debugging'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our serving input function expects the already tokenized representations of the headlines, so we do that pre-processing in the code before calling the REST API.\n",
    "\n",
    "Note: Ideally we would do these transformation in the tensorflow graph directly instead of relying on separate client pre-processing code (see: [training-serving skew](https://developers.google.com/machine-learning/guides/rules-of-ml/#training_serving_skew)), howevever the pre-processing functions we're using are python functions so cannot be embedded in a tensorflow graph. \n",
    "\n",
    "See the <a href=\"../text_classification_native.ipynb\">text_classification_native</a> notebook for a solution to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "import json\n",
    "\n",
    "requests = techcrunch+nytimes+github\n",
    "\n",
    "# Tokenize and pad sentences using same mapping used in the deployed model\n",
    "tokenizer = pickle.load( open( \"txtclsmodel/tokenizer.pickled\", \"rb\" ) )\n",
    "\n",
    "requests_tokenized = tokenizer.texts_to_sequences(requests)\n",
    "requests_tokenized = sequence.pad_sequences(requests_tokenized,maxlen=50)\n",
    "\n",
    "# JSON format the requests\n",
    "request_data = {'instances':requests_tokenized.tolist()}\n",
    "\n",
    "# Authenticate and call CMLE prediction API \n",
    "credentials = GoogleCredentials.get_application_default()\n",
    "api = discovery.build('ml', 'v1', credentials=credentials,\n",
    "            discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "\n",
    "parent = 'projects/%s/models/%s' % (PROJECT, 'txtcls') #version is not specified so uses default\n",
    "response = api.projects().predict(body=request_data, name=parent).execute()\n",
    "\n",
    "# Format and print response\n",
    "for i in range(len(requests)):\n",
    "  print('\\n{}'.format(requests[i]))\n",
    "  print(' github    : {}'.format(response['predictions'][i]['dense_1'][0]))\n",
    "  print(' nytimes   : {}'.format(response['predictions'][i]['dense_1'][1]))\n",
    "  print(' techcrunch: {}'.format(response['predictions'][i]['dense_1'][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of your predictions were correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun with Pre-trained Embedding\n",
    "\n",
    "In the previous model we trained our word embedding from scratch. Often times we get better performance and/or converge faster by leveraging a pre-trained embedding. This is a similar concept to transfer learning during image classification.\n",
    "\n",
    "We will use the popular GloVe embedding which is trained on Wikipedia as well as various news sources like the New York Times.\n",
    "\n",
    "You can read more about Glove at the project homepage: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "You can download the embedding files directly from the stanford.edu site, but we've rehosted it in a GCS bucket for faster download speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://cloud-training-demos/courses/machine_learning/deepdive/09_sequence/text_classification/glove.6B.200d.txt gs://$BUCKET/txtcls/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the embedding is downloaded re-run your cloud training job with the added command line argument: \n",
    "\n",
    "` --embedding_path=gs://${BUCKET}/txtcls/glove.6B.200d.txt`\n",
    "\n",
    "Be sure to change your OUTDIR so it doesn't overwrite the previous model.\n",
    "\n",
    "While the final accuracy may not change significantly, you should notice the model is able to converge to it much more quickly because it no longer has to learn an embedding from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "- This implementation is based on code from: https://github.com/google/eng-edu/tree/master/ml/guides/text_classification.\n",
    "- See the full text classification tutorial at: https://developers.google.com/machine-learning/guides/text-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.py\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import Conv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "CLASSES = {'github': 0, 'nytimes': 1, 'techcrunch': 2}  # label-to-int mapping\n",
    "TOP_K = 20000  # Limit on the number vocabulary size used for tokenization\n",
    "MAX_SEQUENCE_LENGTH = 50  # Sentences will be truncated/padded to this length\n",
    "\n",
    "\"\"\"\n",
    "Helper function to download data from Google Cloud Storage\n",
    "  # Arguments:\n",
    "      source: string, the GCS URL to download from (e.g. 'gs://bucket/file.csv')\n",
    "      destination: string, the filename to save as on local disk. MUST be filename\n",
    "      ONLY, doesn't support folders. (e.g. 'file.csv', NOT 'folder/file.csv')\n",
    "  # Returns: nothing, downloads file to local disk\n",
    "\"\"\"\n",
    "def download_from_gcs(source, destination):\n",
    "    search = re.search('gs://(.*?)/(.*)', source)\n",
    "    bucket_name = search.group(1)\n",
    "    blob_name = search.group(2)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    bucket.blob(blob_name).download_to_filename(destination)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Parses raw tsv containing hacker news headlines and returns (sentence, integer label) pairs\n",
    "  # Arguments:\n",
    "      train_data_path: string, path to tsv containing training data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "      eval_data_path: string, path to tsv containing eval data.\n",
    "        can be a local path or a GCS url (gs://...)\n",
    "  # Returns:\n",
    "      ((train_sentences, train_labels), (test_sentences, test_labels)):  sentences\n",
    "        are lists of strings, labels are numpy integer arrays\n",
    "\"\"\"\n",
    "def load_hacker_news_data(train_data_path, eval_data_path):\n",
    "    if train_data_path.startswith('gs://'):\n",
    "        download_from_gcs(train_data_path, destination='train.csv')\n",
    "        train_data_path = 'train.csv'\n",
    "    if eval_data_path.startswith('gs://'):\n",
    "        download_from_gcs(eval_data_path, destination='eval.csv')\n",
    "        eval_data_path = 'eval.csv'\n",
    "\n",
    "    # Parse CSV using pandas\n",
    "    column_names = ('label', 'text')\n",
    "    df_train = pd.read_csv(train_data_path, names=column_names, sep='\\t')\n",
    "    df_eval = pd.read_csv(eval_data_path, names=column_names, sep='\\t')\n",
    "\n",
    "    return ((list(df_train['text']), np.array(df_train['label'].map(CLASSES))),\n",
    "            (list(df_eval['text']), np.array(df_eval['label'].map(CLASSES))))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create tf.estimator compatible input function\n",
    "  # Arguments:\n",
    "      texts: [strings], list of sentences\n",
    "      labels: numpy int vector, integer labels for sentences\n",
    "      tokenizer: tf.python.keras.preprocessing.text.Tokenizer\n",
    "        used to convert sentences to integers\n",
    "      batch_size: int, number of records to use for each train batch\n",
    "      mode: tf.estimator.ModeKeys.TRAIN or tf.estimator.ModeKeys.EVAL \n",
    "  # Returns:\n",
    "      tf.estimator.inputs.numpy_input_fn, produces feature and label\n",
    "        tensors one batch at a time\n",
    "\"\"\"\n",
    "def input_fn(texts, labels, tokenizer, batch_size, mode):\n",
    "    # Transform text to sequence of integers\n",
    "    x = tokenizer.texts_to_sequences(texts)# TODO (hint: use tokenizer)\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x = sequence.pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)    # TODO (hint: there is a useful function in tf.keras.preprocessing...)\n",
    "\n",
    "    # default settings for training\n",
    "    num_epochs = None\n",
    "    shuffle = True\n",
    "\n",
    "    # override if this is eval\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        num_epochs = 1\n",
    "        shuffle = False\n",
    "\n",
    "    return tf.estimator.inputs.numpy_input_fn(\n",
    "        x, \n",
    "        y=labels,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        shuffle=shuffle,\n",
    "        queue_capacity=50000\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "Builds a CNN model using keras and converts to tf.estimator.Estimator\n",
    "  # Arguments\n",
    "      model_dir: string, file path where training files will be written\n",
    "      config: tf.estimator.RunConfig, specifies properties of tf Estimator\n",
    "      filters: int, output dimension of the layers.\n",
    "      kernel_size: int, length of the convolution window.\n",
    "      embedding_dim: int, dimension of the embedding vectors.\n",
    "      dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "      pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "      embedding_path: string , file location of pre-trained embedding (if used)\n",
    "        defaults to None which will cause the model to train embedding from scratch\n",
    "      word_index: dictionary, mapping of vocabulary to integers. used only if\n",
    "        pre-trained embedding is provided\n",
    "\n",
    "    # Returns\n",
    "        A tf.estimator.Estimator \n",
    "\"\"\"\n",
    "def keras_estimator(model_dir,\n",
    "                    config,\n",
    "                    learning_rate,\n",
    "                    filters=64,\n",
    "                    dropout_rate=0.2,\n",
    "                    embedding_dim=200,\n",
    "                    kernel_size=3,\n",
    "                    pool_size=3,\n",
    "                    embedding_path=None,\n",
    "                    word_index=None):\n",
    "    # Create model instance.\n",
    "    model = models.Sequential()\n",
    "    num_features = min(len(word_index) + 1, TOP_K)\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if embedding_path != None:\n",
    "        embedding_matrix = get_embedding_matrix(word_index, embedding_path, embedding_dim)\n",
    "        is_embedding_trainable = True  # set to False to freeze embedding weights\n",
    "\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Conv1D(filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=pool_size))\n",
    "    model.add(Conv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(len(CLASSES), activation='softmax'))\n",
    "\n",
    "    # Compile model with learning parameters.\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "    estimator = tf.keras.estimator.model_to_estimator(\n",
    "            keras_model=model,\n",
    "            model_dir=model_dir,\n",
    "            config=config )\n",
    "        # TODO: convert keras model to tf.estimator.Estimator\n",
    "\n",
    "    return estimator\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Defines the features to be passed to the model during inference \n",
    "  Expects already tokenized and padded representation of sentences\n",
    "  # Arguments: none\n",
    "  # Returns: tf.estimator.export.ServingInputReceiver\n",
    "\"\"\"\n",
    "def serving_input_fn():\n",
    "    feature_placeholder = tf.placeholder(tf.int16, [None, MAX_SEQUENCE_LENGTH])\n",
    "    features = feature_placeholder  # pass as-is\n",
    "    return tf.estimator.export.TensorServingInputReceiver(features, feature_placeholder)\n",
    "\n",
    "\"\"\"\n",
    "Takes embedding for generic voabulary and extracts the embeddings\n",
    "  matching the current vocabulary\n",
    "  The pre-trained embedding file is obtained from https://nlp.stanford.edu/projects/glove/\n",
    "  # Arguments: \n",
    "      word_index: dict, {key =word in vocabulary: value= integer mapped to that word}\n",
    "      embedding_path: string, location of the pre-trained embedding file on disk\n",
    "      embedding_dim: int, dimension of the embedding space\n",
    "  # Returns: numpy matrix of shape (vocabulary, embedding_dim) that contains the embedded\n",
    "      representation of each word in the vocabulary.\n",
    "\"\"\"\n",
    "def get_embedding_matrix(word_index, embedding_path, embedding_dim):\n",
    "    # Read the pre-trained embedding file and get word to word vector mappings.\n",
    "    embedding_matrix_all = {}\n",
    "\n",
    "    # Download if embedding file is in GCS\n",
    "    if embedding_path.startswith('gs://'):\n",
    "        download_from_gcs(embedding_path, destination='embedding.csv')\n",
    "        embedding_path = 'embedding.csv'\n",
    "\n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:  # Every line contains word followed by the vector value\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_matrix_all[word] = coefs\n",
    "\n",
    "    # Prepare embedding matrix with just the words in our word_index dictionary\n",
    "    num_words = min(len(word_index) + 1, TOP_K)\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= TOP_K:\n",
    "            continue\n",
    "        embedding_vector = embedding_matrix_all.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Main orchestrator. Responsible for calling all other functions in model.py\n",
    "  # Arguments: \n",
    "      output_dir: string, file path where training files will be written\n",
    "      hparams: dict, command line parameters passed from task.py\n",
    "  # Returns: nothing, kicks off training and evaluation\n",
    "\"\"\"\n",
    "def train_and_evaluate(output_dir, hparams):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    \n",
    "    # Load Data\n",
    "    ((train_texts, train_labels), (test_texts, test_labels)) = load_hacker_news_data(\n",
    "        hparams['train_data_path'], hparams['eval_data_path'])\n",
    "\n",
    "    # Create vocabulary from training corpus.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Save token dictionary to use during prediction time\n",
    "    pickle.dump(tokenizer, open('tokenizer.pickled', 'wb'))\n",
    "\n",
    "    # Create estimator\n",
    "    run_config = tf.estimator.RunConfig(save_checkpoints_steps=500)\n",
    "    # TODO: create estimator\n",
    "    estimator = keras_estimator( model_dir=output_dir,\n",
    "                                 config=run_config,\n",
    "                                 learning_rate=hparams['learning_rate'],\n",
    "                                 embedding_path=hparams['embedding_path'], \n",
    "                                 word_index=tokenizer.word_index   )\n",
    "\n",
    "    # Create TrainSpec\n",
    "    train_steps = hparams['num_epochs'] * len(train_texts) / hparams['batch_size']\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn=input_fn(\n",
    "            train_texts,\n",
    "            train_labels,\n",
    "            tokenizer,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.TRAIN),\n",
    "        max_steps=train_steps\n",
    "    )\n",
    "\n",
    "    # Create EvalSpec\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn=input_fn(\n",
    "            test_texts,\n",
    "            test_labels,\n",
    "            tokenizer,\n",
    "            hparams['batch_size'],\n",
    "            mode=tf.estimator.ModeKeys.EVAL),\n",
    "        steps=None,\n",
    "        exporters=exporter,\n",
    "        start_delay_secs=10,\n",
    "        throttle_secs=10\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
